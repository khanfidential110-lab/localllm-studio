name: Build LocalLLM Studio

on:
  push:
    branches: [ main ]
    tags: [ 'v*' ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

jobs:
  build:
    name: Build for ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        include:
          - os: ubuntu-latest
            artifact_name: LocalLLM-Studio-Linux.AppImage
            executable_path: dist/localllm-studio
            asset_content_type: application/octet-stream
            
          - os: windows-latest
            artifact_name: LocalLLM-Studio-Setup.exe
            executable_path: dist/LocalLLM Studio.exe
            asset_content_type: application/vnd.microsoft.portable-executable
            
          - os: macos-latest
            artifact_name: LocalLLM-Studio-macOS.dmg
            executable_path: dist/LocalLLM Studio.app
            asset_content_type: application/x-apple-diskimage

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'
        cache: 'pip'

    # Linux specific setup
    - name: Install Linux Dependencies
      if: runner.os == 'Linux'
      run: |
        sudo apt-get update
        sudo apt-get install -y libwebkit2gtk-4.1-dev \
          build-essential \
          libasound2-dev \
          libssl-dev \
          cmake
          
    # macOS specific setup
    - name: Install macOS Dependencies
      if: runner.os == 'macOS'
      run: |
        brew list cmake || brew install cmake

    - name: Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pyinstaller

    # Special handling for llama-cpp-python on different platforms
    - name: Install llama-cpp-python (Linux/Mac)
      if: runner.os != 'Windows'
      run: |
        CMAKE_ARGS="-DLLAMA_BLAS=ON -DLLAMA_VENDOR=Generic" pip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir

    - name: Install llama-cpp-python (Windows)
      if: runner.os == 'Windows'
      run: |
        # Use pre-built wheels for Windows to avoid compilation
        pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu

    - name: Build with PyInstaller
      run: |
        pyinstaller localllm_studio.spec

    # Packaging for Linux (AppImage) - Simplified: just tar or keep binary for now
    # Ideally we'd use appimagetool, but for "hustle free" let's just zip the binary or folder
    - name: Package Linux
      if: runner.os == 'Linux'
      run: |
        mv "dist/localllm-studio" "dist/LocalLLM-Studio-Linux"
        chmod +x "dist/LocalLLM-Studio-Linux"

    # Upload Artifacts (for workflow run)
    - name: Upload Artifact
      uses: actions/upload-artifact@v4
      with:
        name: ${{ matrix.artifact_name }}
        path: dist/

    # Create Release (only on tag)
    - name: Release
      uses: softprops/action-gh-release@v1
      if: startsWith(github.ref, 'refs/tags/')
      with:
        files: |
          dist/*
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
